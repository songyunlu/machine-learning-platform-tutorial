{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# parameters that will be injected by machine learning platform\n",
    "project_id = \"457e13bc-d47e-4a2e-be13-bcd47eba2ea5\"\n",
    "training_id = \"c81fbe3a-a06a-4704-9fbe-3aa06a07047c\"\n",
    "data_set_source = [ \"iris.csv\" ]\n",
    "feature_set = {\n",
    "  \"sepal width (cm)\": {\n",
    "    \"type\": \"string\",\n",
    "    \"description\": None\n",
    "  },\n",
    "  \"target_iris_type\": {\n",
    "    \"type\": \"string\",\n",
    "    \"description\": None\n",
    "  },\n",
    "  \"petal width (cm)\": {\n",
    "    \"type\": \"string\",\n",
    "    \"description\": None\n",
    "  },\n",
    "  \"petal length (cm)\": {\n",
    "    \"type\": \"string\",\n",
    "    \"description\": None\n",
    "  },\n",
    "  \"sepal length (cm)\": {\n",
    "    \"type\": \"string\",\n",
    "    \"description\": None\n",
    "  }\n",
    "}\n",
    "output_dir = \"out\"\n",
    "training_metrics_file = \"training.metrics\"\n",
    "cross_validation_metrics_file = \"cross_validation.metrics\"\n",
    "testing_metrics_file = \"testing.metrics\"\n",
    "feature_importance_file = \"feature.importance\"\n",
    "model_file = \"model.pkl\"\n",
    "metrics_feedback_url = f\"http://localhost:8080/projects/{project_id}/trainings/{training_id}/metrics\"\n",
    "test_data_proportion = 0.4\n",
    "num_neighbors = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"project_id =\", project_id)\n",
    "print(\"training_id =\", training_id)\n",
    "print(\"data_set_source =\", data_set_source)\n",
    "print(\"output_dir\", output_dir)\n",
    "print(\"training_metrics_file\", training_metrics_file)\n",
    "print(\"cross_validation_metrics_file\", cross_validation_metrics_file)\n",
    "print(\"testing_metrics_file\", testing_metrics_file)\n",
    "print(\"model_file\", model_file)\n",
    "print(\"metrics_feedback_url =\", metrics_feedback_url)\n",
    "print(\"test_data_proportion =\", test_data_proportion)\n",
    "print(\"num_neighbors =\", num_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that prints the iris' classification based on the algorithm's output\n",
    "def classifyiris(z):\n",
    "    if z[0] == 0:\n",
    "        print(\"The iris is setosa.\\n\")\n",
    "    elif z[0] == 1:\n",
    "        print(\"The iris is versicolor.\\n\")\n",
    "    else:\n",
    "        print(\"The iris is virginica.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "f = open(data_set_source[0])\n",
    "f.readline()  # skip the header\n",
    "iris = np.loadtxt(fname = f, delimiter = ',')\n",
    "iris_data = iris[:, 0:4]\n",
    "iris_target = iris[:, 4:5].reshape(-1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris_data, \n",
    "    iris_target, \n",
    "    test_size=test_data_proportion, \n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# using the K Nearest Neighbor Algorithm\n",
    "knn = KNeighborsClassifier(n_neighbors = num_neighbors)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "z = knn.predict([[3,5,4,2]])\n",
    "print(\"Using the k nearest neighbor algorithm =\", knn.predict([[3,5,4,2]]))\n",
    "classifyiris(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# get training metrics\n",
    "z = knn.predict(X_train)\n",
    "training_accuracy = metrics.accuracy_score(z, y_train)\n",
    "training_precision_macro = metrics.precision_score(z, y_train, average='macro')\n",
    "training_recall_macro = metrics.recall_score(y_train, z, average='macro')\n",
    "training_f1_score_macro = metrics.f1_score(y_train, z, average='macro')\n",
    "training_confusion_matrix = metrics.confusion_matrix(y_train, z)\n",
    "print(\"training accuracy =\", training_accuracy)\n",
    "print(\"training precision =\", training_precision_macro)\n",
    "print(\"training recall =\", training_recall_macro)\n",
    "print(\"training f1 score =\", training_f1_score_macro)\n",
    "print(\"training confusion matrix = \\n\", training_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# saving training metrics to machine learning platform\n",
    "training_confusion_matrix_string = np.array2string(training_confusion_matrix, separator=',')\n",
    "\n",
    "metrics_payload = {\n",
    "    'type': 'TRAINING',\n",
    "    'accuracy': training_accuracy,\n",
    "    'precisionMacro': training_precision_macro,\n",
    "    'recallMacro': training_recall_macro,\n",
    "    'f1Macro': training_f1_score_macro,\n",
    "    'confusionMatrix': training_confusion_matrix_string\n",
    "}\n",
    "\n",
    "with open(training_metrics_file, 'w') as metrics_out:\n",
    "    json.dump(metrics_payload, metrics_out, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cross validation metrics\n",
    "z = knn.predict(X_test)\n",
    "cv_accuracy = metrics.accuracy_score(z, y_test)\n",
    "cv_precision_macro = metrics.precision_score(z, y_test, average='macro')\n",
    "cv_recall_macro = metrics.recall_score(y_test, z, average='macro')\n",
    "cv_f1_score_macro = metrics.f1_score(y_test, z, average='macro')\n",
    "cv_confusion_matrix = metrics.confusion_matrix(y_test, z)\n",
    "print(\"cross validation accuracy\", cv_accuracy)\n",
    "print(\"cross validation precision =\", cv_precision_macro)\n",
    "print(\"cross validation recall\", cv_recall_macro)\n",
    "print(\"cross validation f1 score =\", cv_f1_score_macro)\n",
    "print(\"training confusion matrix = \\n\", cv_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving cross validation metrics to machine learning platform\n",
    "cv_confusion_matrix_string = np.array2string(cv_confusion_matrix, separator=',')\n",
    "\n",
    "metrics_payload = {\n",
    "    'type': 'CROSS_VALIDATION',\n",
    "    'accuracy': cv_accuracy,\n",
    "    'precisionMacro': cv_precision_macro,\n",
    "    'recallMacro': cv_recall_macro,\n",
    "    'f1Macro': cv_f1_score_macro,\n",
    "    'confusionMatrix': cv_confusion_matrix_string\n",
    "}\n",
    "\n",
    "with open(cross_validation_metrics_file, 'w') as metrics_out:\n",
    "    json.dump(metrics_payload, metrics_out, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# saving feature importance\n",
    "feature_importance = {}\n",
    "for feature_name in feature_set.keys():\n",
    "    feature_importance[feature_name] = random.uniform(0.0, 1.0)\n",
    "\n",
    "with open(feature_importance_file, 'w') as feature_importance_out:\n",
    "    json.dump(feature_importance, feature_importance_out, ensure_ascii=False, indent=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "from joblib import dump\n",
    "\n",
    "dump(knn, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# saving arbitrary output files.\n",
    "with open(f'{output_dir}/testing.json', 'w') as testing_json:\n",
    "    json.dump(metrics_payload, testing_json, ensure_ascii=False, indent=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Iris-Classification-with-scikit-learn",
   "language": "python",
   "name": "iris-classification-with-scikit-learn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}